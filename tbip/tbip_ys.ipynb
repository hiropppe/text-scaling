{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3c97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d38e5f",
   "metadata": {},
   "source": [
    "## Testing Text-Based Ideal Points for Affective News\n",
    "\n",
    "政治学分野の感情極性公開データ (Lori Young and Stuart Soroka. Affective News: The Automated Coding of Sentiment in Political Texts. Political\n",
    "Communication, 29(2):205–231, 2012.) による評価\n",
    "\n",
    "- 理想点モデル（極性軸=政治的立ち位置を想定）なので評価データとしてはちょっと微妙だがお試し程度に実行\n",
    "- 著者属性がないので、1文書=1著者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2cd6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install numpyro\n",
    "%pip install optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46cea5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install watermark\n",
    "%load_ext watermark\n",
    "%watermark -u -d -p torch,jax,numpyro,optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a53619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!ls /content/drive/MyDrive/Develop/datasets/tbip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80941f40",
   "metadata": {},
   "source": [
    "## Hyperparameters and Initialization\n",
    "\n",
    "We start setting some hyperparameters. We fix the number of topics $K$. We also set a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6aba9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "num_topics = 10\n",
    "rng_seed = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c14f7a",
   "metadata": {},
   "source": [
    "## データ準備\n",
    "New York Times 900 記事、半分が 1988–2008 年の経済面から, 残りの半分は、2007–2009 年の環境・外交・犯罪に関する一面記事からランダムにサンプリングされたもの。 \n",
    "各記事は専門の評価者 3 人により、論調について Positive/Negative/Neutral のタグが付与されている. \n",
    "この 3 個のタグを下記ののルールで 1–5 の 5 段階に分けしたもの\n",
    "| 条件 | スケール |\n",
    "| ---- | ---- |\n",
    "| #(positive) = 3 | 5 |\n",
    "| #(positive) = 2 | 4 |\n",
    "| それ以外 | 3 | \n",
    "| #(negative) = 2 | 2 |\n",
    "| #(negative) = 3 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8acc4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Develop/datasets/tbip/YS.2012.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93ded8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "# 1文書１著者として扱う\n",
    "author_indices = jax.device_put(df.index.values, jax.devices('gpu')[0])\n",
    "counts = sparse.csr_matrix(df.iloc[:, 5:].values)\n",
    "\n",
    "vocabulary = list(df.columns[5:])\n",
    "author_map = np.array(df[\"doc_id\"].values)\n",
    "\n",
    "num_authors = int(author_indices.max() + 1)\n",
    "num_documents, num_words = counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a870911",
   "metadata": {},
   "source": [
    "[In the paper](https://www.aclweb.org/anthology/2020.acl-main.475/), the parameters are pre-initialized with [Poisson factorization](https://arxiv.org/abs/1311.1704). Most of the time, we find this doesn't make a big difference for the learned ideal points, but it helps to interpret the ideological topics. \n",
    "\n",
    "Below, we initialize with Scikit-Learn's non-negative matrix factorization (NMF) implementation. Although we find that Poisson factorization learns more interpretable topics, we use Scikit-Learn's NMF implementation here because it is faster. To use Poisson factorization, see the [code in the Github repo](https://github.com/keyonvafa/tbip/blob/master/setup/poisson_factorization.py). \n",
    "\n",
    "If you would like to skip this pre-initialization step, set `pre_initialize_parameters = False` in the cell below. (Pre-initialization is recommended.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ea5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_initialize_parameters = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97adcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit NMF to be used as initialization for TBIP\n",
    "from sklearn.decomposition import NMF\n",
    "from jax import numpy as jnp\n",
    "\n",
    "if pre_initialize_parameters:\n",
    "    nmf_model = NMF(\n",
    "        n_components=num_topics, init=\"random\", random_state=0, max_iter=500\n",
    "    )\n",
    "    # Define initialization arrays\n",
    "    initial_document_loc = jnp.log(\n",
    "        jnp.array(np.float32(nmf_model.fit_transform(counts) + 1e-2))\n",
    "    )\n",
    "    initial_objective_topic_loc = jnp.log(\n",
    "        jnp.array(np.float32(nmf_model.components_ + 1e-2))\n",
    "    )\n",
    "else:\n",
    "    rng1, rng2 = random.split(rng_seed, 2)\n",
    "    initial_document_loc = random.normal(rng1, shape=(num_documents, num_topics))\n",
    "    initial_objective_topic_loc = random.normal(rng2, shape=(num_topics, num_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ffbf1",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "\n",
    "We perform inference using [variational inference](https://arxiv.org/abs/1601.00670) with [reparameterization](https://arxiv.org/abs/1312.6114) [gradients](https://arxiv.org/abs/1401.4082). We provide a brief summary below, but encourage readers to [refer to the original paper](https://www.aclweb.org/anthology/2020.acl-main.475/) for a more complete overview.\n",
    "\n",
    "It is intractable to evaluate the posterior distribution $p(\\theta, \\beta, \\eta, x | y)$, so we approximate the posterior with a distribution $q_\\phi(\\theta, \\beta,\\eta,x)$, parameterized by $\\phi$. How do we set the values $\\phi$? We want to minimize the KL-Divergence between $q$ and the posterior, which is equivalent to maximizing the ELBO:\n",
    "$$\\mathbb{E}_{q_\\phi}[\\log p(y, \\theta, \\beta, \\eta, x) - \\log q_{\\phi}(\\theta, \\beta, \\eta, x)].$$\n",
    "We set the variational family to be the mean-field family, meaning the latent variables factorize over documents $d$, topics $k$, and authors $s$:\n",
    "$$q_\\phi(\\theta, \\beta, \\eta, x) = \\prod_{d,k,s} q(\\theta_d)q(\\beta_k)q(\\eta_k)q(x_s).$$\n",
    "We use lognormal factors for the positive variables and Gaussian factors for the real variables:\n",
    "$$q(\\theta_{dk}) = \\text{LogNormal}(\\mu_{\\theta_{dk}}\\sigma^2_{\\theta_{dk}})$$\n",
    "$$q(\\beta{dv}) = \\text{LogNormal}(\\mu_{\\beta_{kv}}, \\sigma^2_{\\beta_{kv}})$$\n",
    "$$q(\\eta_{kv}) = \\text{Normal}(\\mu_{\\eta_{kv}}, \\sigma^2_{\\eta_{kv}})$$\n",
    "$$q(x_s) = \\text{Normal}(\\mu_{x_s}, \\sigma^2_{x_s}).$$\n",
    "\n",
    "Thus, our goal is to maximize the ELBO with respect to $\\phi = \\{\\mu_\\theta, \\sigma_\\theta, \\mu_\\beta, \\sigma_\\beta,\\mu_\\eta, \\sigma_\\eta, \\mu_x, \\sigma_x\\}$. \n",
    "\n",
    "In the cell below, we define the model and the variational family (guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44ad43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro import param, plate, sample\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.distributions import constraints\n",
    "\n",
    "# Define the model and variational family\n",
    "\n",
    "\n",
    "class TBIP:\n",
    "    def __init__(self, N, D, K, V, batch_size, init_mu_theta=None, init_mu_beta=None):\n",
    "        self.N = N  # number of people\n",
    "        self.D = D  # number of documents\n",
    "        self.K = K  # number of topics\n",
    "        self.V = V  # number of words in vocabulary\n",
    "        self.batch_size = batch_size  # number of documents in a batch\n",
    "\n",
    "        if init_mu_theta is None:\n",
    "            init_mu_theta = jnp.zeros([D, K])\n",
    "        else:\n",
    "            self.init_mu_theta = init_mu_theta\n",
    "\n",
    "        if init_mu_beta is None:\n",
    "            init_mu_beta = jnp.zeros([K, V])\n",
    "        else:\n",
    "            self.init_mu_beta = init_mu_beta\n",
    "\n",
    "    def model(self, Y_batch, d_batch, i_batch):\n",
    "        with plate(\"i\", self.N):\n",
    "            # Sample the per-unit latent variables (ideal points)\n",
    "            x = sample(\"x\", dist.Normal())\n",
    "\n",
    "        with plate(\"k\", size=self.K, dim=-2):\n",
    "            with plate(\"k_v\", size=self.V, dim=-1):\n",
    "                beta = sample(\"beta\", dist.Gamma(0.3, 0.3))\n",
    "                eta = sample(\"eta\", dist.Normal())\n",
    "\n",
    "        with plate(\"d\", size=self.D, subsample_size=self.batch_size, dim=-2):\n",
    "            with plate(\"d_k\", size=self.K, dim=-1):\n",
    "                # Sample document-level latent variables (topic intensities)\n",
    "                theta = sample(\"theta\", dist.Gamma(0.3, 0.3))\n",
    "\n",
    "            # Compute Poisson rates for each word\n",
    "            P = jnp.sum(\n",
    "                jnp.expand_dims(theta, 2)\n",
    "                * jnp.expand_dims(beta, 0)\n",
    "                * jnp.exp(\n",
    "                    jnp.expand_dims(x[i_batch], (1, 2)) * jnp.expand_dims(eta, 0)\n",
    "                ),\n",
    "                1,\n",
    "            )\n",
    "\n",
    "            with plate(\"v\", size=self.V, dim=-1):\n",
    "                # Sample observed words\n",
    "                sample(\"Y_batch\", dist.Poisson(P), obs=Y_batch)\n",
    "\n",
    "    def guide(self, Y_batch, d_batch, i_batch):\n",
    "        # This defines variational family. Notice that each of the latent variables\n",
    "        # defined in the sample statements in the model above has a corresponding\n",
    "        # sample statement in the guide. The guide is responsible for providing\n",
    "        # variational parameters for each of these latent variables.\n",
    "\n",
    "        # Also notice it is required that model and the guide have the same call.\n",
    "\n",
    "        mu_x = param(\n",
    "            \"mu_x\", init_value=-1 + 2 * random.uniform(random.PRNGKey(1), (self.N,))\n",
    "        )\n",
    "        sigma_x = param(\n",
    "            \"sigma_y\", init_value=jnp.ones([self.N]), constraint=constraints.positive\n",
    "        )\n",
    "\n",
    "        mu_eta = param(\n",
    "            \"mu_eta\", init_value=random.normal(random.PRNGKey(2), (self.K, self.V))\n",
    "        )\n",
    "        sigma_eta = param(\n",
    "            \"sigma_eta\",\n",
    "            init_value=jnp.ones([self.K, self.V]),\n",
    "            constraint=constraints.positive,\n",
    "        )\n",
    "\n",
    "        mu_theta = param(\"mu_theta\", init_value=self.init_mu_theta)\n",
    "        sigma_theta = param(\n",
    "            \"sigma_theta\",\n",
    "            init_value=jnp.ones([self.D, self.K]),\n",
    "            constraint=constraints.positive,\n",
    "        )\n",
    "\n",
    "        mu_beta = param(\"mu_beta\", init_value=self.init_mu_beta)\n",
    "        sigma_beta = param(\n",
    "            \"sigma_beta\",\n",
    "            init_value=jnp.ones([self.K, self.V]),\n",
    "            constraint=constraints.positive,\n",
    "        )\n",
    "\n",
    "        with plate(\"i\", self.N):\n",
    "            sample(\"x\", dist.Normal(mu_x, sigma_x))\n",
    "\n",
    "        with plate(\"k\", size=self.K, dim=-2):\n",
    "            with plate(\"k_v\", size=self.V, dim=-1):\n",
    "                sample(\"beta\", dist.LogNormal(mu_beta, sigma_beta))\n",
    "                sample(\"eta\", dist.Normal(mu_eta, sigma_eta))\n",
    "\n",
    "        with plate(\"d\", size=self.D, subsample_size=self.batch_size, dim=-2):\n",
    "            with plate(\"d_k\", size=self.K, dim=-1):\n",
    "                sample(\"theta\", dist.LogNormal(mu_theta[d_batch], sigma_theta[d_batch]))\n",
    "\n",
    "    def get_batch(self, rng, Y, author_indices):\n",
    "        # Helper functions to obtain a batch of data, convert from scipy.sparse\n",
    "        # to jax.numpy.array and move to gpu\n",
    "\n",
    "        D_batch = random.choice(rng, jnp.arange(self.D), shape=(self.batch_size,))\n",
    "        Y_batch = jax.device_put(jnp.array(Y[D_batch].toarray()), jax.devices(\"gpu\")[0])\n",
    "        D_batch = jax.device_put(D_batch, jax.devices(\"gpu\")[0])\n",
    "        I_batch = author_indices[D_batch]\n",
    "        return Y_batch, I_batch, D_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7de9d",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Below we initialize an instance of the TBIP model, and associated SVI object. The latter is used to compute the Evidence Lower Bound (ELBO) given current value of guide's parameters and current batch of data.\n",
    "\n",
    "We optimize the model using Adam with exponential decay of learning rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "944ba50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "from jax import jit\n",
    "from optax import adam, exponential_decay\n",
    "\n",
    "from numpyro.infer import SVI, TraceMeanField_ELBO\n",
    "\n",
    "num_steps = 10000\n",
    "batch_size = 512  # Large batches are recommended\n",
    "learning_rate = 0.01\n",
    "decay_rate = 0.01\n",
    "\n",
    "tbip = TBIP(\n",
    "    N=num_authors,\n",
    "    D=num_documents,\n",
    "    K=num_topics,\n",
    "    V=num_words,\n",
    "    batch_size=batch_size,\n",
    "    init_mu_theta=initial_document_loc,\n",
    "    init_mu_beta=initial_objective_topic_loc,\n",
    ")\n",
    "\n",
    "svi_batch = SVI(\n",
    "    model=tbip.model,\n",
    "    guide=tbip.guide,\n",
    "    optim=adam(exponential_decay(learning_rate, num_steps, decay_rate)),\n",
    "    loss=TraceMeanField_ELBO(),\n",
    ")\n",
    "\n",
    "# Compile update function for faster training\n",
    "svi_batch_update = jit(svi_batch.update)\n",
    "\n",
    "# Get initial batch. This informs the dimension of arrays and ensures they are\n",
    "# consistent with dimensions (N, D, K, V) defined above.\n",
    "Y_batch, I_batch, D_batch = tbip.get_batch(random.PRNGKey(1), counts, author_indices)\n",
    "\n",
    "# Initialize the parameters using initial batch\n",
    "svi_state = svi_batch.init(\n",
    "    random.PRNGKey(0), Y_batch=Y_batch, d_batch=D_batch, i_batch=I_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run this cell to create helper function for printing topics\n",
    "\n",
    "\n",
    "def get_topics(\n",
    "    neutral_mean, negative_mean, positive_mean, vocabulary, print_to_terminal=True\n",
    "):\n",
    "    num_topics, num_words = neutral_mean.shape\n",
    "    words_per_topic = 10\n",
    "    top_neutral_words = np.argsort(-neutral_mean, axis=1)\n",
    "    top_negative_words = np.argsort(-negative_mean, axis=1)\n",
    "    top_positive_words = np.argsort(-positive_mean, axis=1)\n",
    "    topic_strings = []\n",
    "    for topic_idx in range(num_topics):\n",
    "        neutral_start_string = \"Neutral  {}:\".format(topic_idx)\n",
    "        neutral_row = [\n",
    "            vocabulary[word] for word in top_neutral_words[topic_idx, :words_per_topic]\n",
    "        ]\n",
    "        neutral_row_string = \", \".join(neutral_row)\n",
    "        neutral_string = \" \".join([neutral_start_string, neutral_row_string])\n",
    "\n",
    "        positive_start_string = \"Positive {}:\".format(topic_idx)\n",
    "        positive_row = [\n",
    "            vocabulary[word] for word in top_positive_words[topic_idx, :words_per_topic]\n",
    "        ]\n",
    "        positive_row_string = \", \".join(positive_row)\n",
    "        positive_string = \" \".join([positive_start_string, positive_row_string])\n",
    "\n",
    "        negative_start_string = \"Negative {}:\".format(topic_idx)\n",
    "        negative_row = [\n",
    "            vocabulary[word] for word in top_negative_words[topic_idx, :words_per_topic]\n",
    "        ]\n",
    "        negative_row_string = \", \".join(negative_row)\n",
    "        negative_string = \" \".join([negative_start_string, negative_row_string])\n",
    "\n",
    "        if print_to_terminal:\n",
    "            topic_strings.append(negative_string)\n",
    "            topic_strings.append(neutral_string)\n",
    "            topic_strings.append(positive_string)\n",
    "            topic_strings.append(\"==========\")\n",
    "        else:\n",
    "            topic_strings.append(\n",
    "                \"  \\n\".join([negative_string, neutral_string, positive_string])\n",
    "            )\n",
    "\n",
    "    if print_to_terminal:\n",
    "        all_topics = \"{}\\n\".format(np.array(topic_strings))\n",
    "    else:\n",
    "        all_topics = np.array(topic_strings)\n",
    "    return all_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82301c7",
   "metadata": {},
   "source": [
    "## Execute Training\n",
    "The code above was creating the model; below we actually run training. You can adjust the number of steps to train (`num_steps`, defined above) and the frequency at which to print the ELBO (`print_steps`, defined below).\n",
    "\n",
    "Here, we run our training loop. Topic summaries and ordered ideal points will print every 2500 steps. Typically in our experiments it takes 15,000 steps or so to begin seeing sensible results, but of course this depends on the corpus. These sensible results should be reached within a half hour. For the default corpus of Senate speeches, it should take less than 2 hours to complete 30,000 training steps (which is the default `num_steps`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eca9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SVI\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "print_steps = 100\n",
    "print_intermediate_results = False\n",
    "\n",
    "rngs = random.split(random.PRNGKey(2), num_steps)\n",
    "losses = []\n",
    "pbar = tqdm(range(num_steps))\n",
    "\n",
    "\n",
    "for step in pbar:\n",
    "    Y_batch, I_batch, D_batch = tbip.get_batch(rngs[step], counts, author_indices)\n",
    "    svi_state, loss = svi_batch_update(\n",
    "        svi_state, Y_batch=Y_batch, d_batch=D_batch, i_batch=I_batch\n",
    "    )\n",
    "\n",
    "    loss = loss / counts.shape[0]\n",
    "    losses.append(loss)\n",
    "    if step % print_steps == 0 or step == num_steps - 1:\n",
    "        pbar.set_description(\n",
    "            \"Init loss: \"\n",
    "            + \"{:10.4f}\".format(jnp.array(losses[0]))\n",
    "            + f\"; Avg loss (last {print_steps} iter): \"\n",
    "            + \"{:10.4f}\".format(jnp.array(losses[-100:]).mean())\n",
    "        )\n",
    "\n",
    "    if (step + 1) % 2500 == 0 or step == num_steps - 1:\n",
    "        # Save intermediate results\n",
    "        estimated_params = svi_batch.get_params(svi_state)\n",
    "\n",
    "        neutral_mean = (\n",
    "            estimated_params[\"mu_beta\"] + estimated_params[\"sigma_beta\"] ** 2 / 2\n",
    "        )\n",
    "\n",
    "        positive_mean = (\n",
    "            estimated_params[\"mu_beta\"]\n",
    "            + estimated_params[\"mu_eta\"]\n",
    "            + (estimated_params[\"sigma_beta\"] ** 2 + estimated_params[\"sigma_eta\"] ** 2)\n",
    "            / 2\n",
    "        )\n",
    "\n",
    "        negative_mean = (\n",
    "            estimated_params[\"mu_beta\"]\n",
    "            - estimated_params[\"mu_eta\"]\n",
    "            + (estimated_params[\"sigma_beta\"] ** 2 + estimated_params[\"sigma_eta\"] ** 2)\n",
    "            / 2\n",
    "        )\n",
    "\n",
    "        np.save(\"neutral_topic_mean.npy\", neutral_mean)\n",
    "        np.save(\"negative_topic_mean.npy\", positive_mean)\n",
    "        np.save(\"positive_topic_mean.npy\", negative_mean)\n",
    "\n",
    "        topics = get_topics(neutral_mean, positive_mean, negative_mean, vocabulary)\n",
    "\n",
    "        with open(\"topics.txt\", \"w\") as f:\n",
    "            print(topics, file=f)\n",
    "\n",
    "        authors = pd.DataFrame(\n",
    "            {\"name\": author_map, \"ideal_point\": np.array(estimated_params[\"mu_x\"])}\n",
    "        )\n",
    "        authors.to_csv(\"authors.csv\")\n",
    "\n",
    "        if print_intermediate_results:\n",
    "            print(f\"Results after {step} steps.\")\n",
    "            print(topics)\n",
    "            sorted_authors = \"Authors sorted by their ideal points: \" + \",\".join(\n",
    "                list(authors.sort_values(\"ideal_point\")[\"name\"])\n",
    "            )\n",
    "            print(sorted_authors.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "neutral_topic_mean = np.load(\"neutral_topic_mean.npy\")\n",
    "negative_topic_mean = np.load(\"negative_topic_mean.npy\")\n",
    "positive_topic_mean = np.load(\"positive_topic_mean.npy\")\n",
    "authors = pd.read_csv(\"authors.csv\")\n",
    "authors[\"name\"] = authors[\"name\"].str.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5b7f4",
   "metadata": {},
   "source": [
    "For example, here is a graph of the learned ideal points. We don't label each point because there are too many to plot. Below we select some authors to label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ec0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人手評価との相関\n",
    "from scipy.stats import pearsonr\n",
    "#ideal_point = np.array(svi_batch.get_params(svi_state)[\"mu_x\"])\n",
    "ideal_point = authors[\"ideal_point\"].values\n",
    "human_scale = df['ys_scale'].values\n",
    "correlation, p_value = pearsonr(ideal_point, human_scale)\n",
    "print(f\"Pearson correlation: {correlation:.4f} (p={p_value:.2e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea85262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'scale': human_scale,\n",
    "    'ideal_point': ideal_point\n",
    "})\n",
    "\n",
    "sns.boxplot(data=result_df, x='scale', y='ideal_point', ax=ax)\n",
    "sns.stripplot(data=result_df, x='scale', y='ideal_point', ax=ax, alpha=0.3, size=3)\n",
    "ax.set_title(f'TBIP (NumPyro SVI) - corr = {correlation:.3f}')\n",
    "ax.set_xlabel('Human Scale')\n",
    "ax.set_ylabel('Estimated θ')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add9c979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
