⏺EM アルゴリズム?
論文では EM と書かれているが、厳密な EM ではなく、**Block Coordinate Descent（ブロック座標降下法）**と呼ばれる手法に近い

  while not_converged:
      # ω, α を固定して β, ψ を最適化
      for j in range(n_words):
          optimize beta_j, psi_j

      # β, ψ を固定して ω, α を最適化
      for i in range(n_docs):
          optimize omega_i, alpha_i

  1. 凸最適化の理論

  各ブロック（ω,α固定時のβ,ψ、またはその逆）での最適化は局所的には凸問題に近いため、交互最適化で全体の尤度が単調増加

  L(θ) = log P(Y | α, ω, β, ψ)

  ∂L/∂ω_i = 0, ∂L/∂α_i = 0  (他を固定)
  ∂L/∂β_j = 0, ∂L/∂ψ_j = 0  (他を固定)

  これらの偏微分方程式を交互に解くことで、全体の尤度を最大化

  2. 収束の保証

  - 各ステップで尤度が改善される（または変わらない）
  - 尤度は上に有界
  - → 局所最適解への収束が保証される

  3. なぜ「EM」と呼ばれるのか？

  厳密な？ EM ではないが以下の理由で「EM的」と言える：

  EMの一般化された解釈

  - Eステップに相当: パラメータの一部を固定（条件付け）
  - Mステップに相当: 条件付き尤度を最大化

  コードの妥当性

  1. 各最適化で scipy.optimize.minimize を使用
     → 局所的には最適解を求めている

  2. 収束判定
     diffllik = (LL_new - LL_old) / |LL_old|
     → 尤度の改善が十分小さくなったら終了

  3. 識別制約（標準化・符号固定）
     → パラメータ空間を適切に制限

  まとめ

  この手法は：
  - ✅ 数学的に合理的（座標降下法の理論に基づく）
  - ✅ 収束が保証される（局所最適解へ）
  - ⚠️ 厳密なEMではない（潜在変数の周辺化がない）
  - ⚠️ 大域的最適解は保証されない（初期値依存）


⏺ パラメータの役割分担

  WORDFISHモデルでは：

  λ_ij = exp(α_i + ψ_j + ω_i * β_j)
             ↑     ↑     ↑       ↑
          文書FE 単語FE 文書位置 単語重み

  ペアの意味

  1. (ω_i, α_i): 文書iに関するパラメータ
    - ω_i: 文書の潜在位置（推定したい主要パラメータ）
    - α_i: 文書の固定効果（文書の総単語数の調整）
  2. (β_j, ψ_j): 単語jに関するパラメータ
    - β_j: 単語の識別力（位置推定への寄与）
    - ψ_j: 単語の固定効果（単語の全体的な頻度調整）

  2つずつ最適化する理由

  1. 識別可能性（Identifiability）

  ω_i と α_i は密接に関連している：

  # もし α_i だけを最適化すると：
  λ_ij = exp(α_i + ψ_j + ω_i * β_j)
       = exp(α_i) * exp(ψ_j + ω_i * β_j)

  # ω_i を動かすと α_i も影響を受ける
  # → 同時に最適化しないとバランスが崩れる

  2. 尤度関数の形状

  各文書 i について：

  L_i(ω_i, α_i | β, ψ) = Σ_j [-λ_ij + y_ij * log(λ_ij)]
                        = Σ_j [-exp(α_i + ψ_j + ω_i*β_j)
                             + y_ij * (α_i + ψ_j + ω_i*β_j)]

  この式ではω_i と α_i が同じ尤度関数に現れるため、同時最適化が効率的

  3. 具体例で見る相互依存性

  # 文書iの観測データ: y_i = [5, 10, 3]
  # 固定されたパラメータ: β = [1, -1, 0], ψ = [2, 2, 2]

  # ω_i だけを最適化しようとすると：
  λ_ij = exp(α_i + 2 + ω_i * β_j)
  # α_i が固定だと、ω_i の最適値が制約される

  # (ω_i, α_i) を同時に最適化すると：
  λ_ij = exp(α_i + 2 + ω_i * β_j)
  # 自由度が2つあり、より柔軟にデータにフィットできる

  もし1つずつ最適化したら？

  # 仮想的な1つずつの最適化
  for i in range(n_docs):
      optimize omega_i  # α_i 固定
      optimize alpha_i  # ω_i 固定（今変えたばかり！）

  # 問題点：
  # - 収束が遅くなる（ジグザグに進む）
  # - 不安定になる可能性（振動）
  # - 計算コストが増える（最適化の回数が2倍）

  コードでの実装

  # 最初の文書（α_0 = 0 制約あり）
  res = minimize(
      self._llik_alpha_omega_first,
      x0=params['omega'][0],  # ω_0 のみ最適化
      args=(dta[0, :], params['beta'], params['psi']),
      method='BFGS'
  )

  # 他の文書（両方最適化）
  res = minimize(
      self._llik_alpha_omega,
      x0=[params['omega'][i], params['alpha'][i]],  # 両方同時
      args=(dta[i, :], params['beta'], params['psi']),
      method='BFGS'
  )

  理論的根拠

  この2つずつの最適化は、条件付き独立性に基づいている：

  P(Y | θ) = Π_i Π_j P(y_ij | α_i, ω_i, β_j, ψ_j)

  # β, ψ を固定すると：
  P(Y | α, ω, β*, ψ*) = Π_i [Π_j P(y_ij | α_i, ω_i, β*, ψ*)]
                                ↑
                      文書iに関する項のみ（独立）

  # したがって、各文書について独立に (α_i, ω_i) を最適化できる

  まとめ

  2つずつペアで最適化するのは：

  1. モデルの構造: 文書パラメータと単語パラメータの自然な分離
  2. 識別可能性: ペア内のパラメータの相互依存関係
  3. 効率性: 同時最適化による高速な収束
  4. 安定性: 振動を避けたスムーズな更新


⏺ ポアソン分布の対数尤度の導出

  ポアソン分布の確率質量関数

  ポアソン分布 Poisson(λ) に従う確率変数 Y の確率質量関数は：

  P(Y = y | λ) = (λ^y * e^(-λ)) / y!

  ここで、y は観測された単語の出現回数、λ は期待値（平均出現回数）

  対数尤度への変換

  この確率質量関数の対数を取る：

  log P(Y = y | λ) = log(λ^y * e^(-λ) / y!)
                   = log(λ^y) + log(e^(-λ)) - log(y!)
                   = y * log(λ) - λ - log(y!)

  最適化における定数項の省略

  最適化（パラメータ推定）では、パラメータ λ に依存しない項は無視できる
  log(y!) は y（観測データ）にのみ依存し、λ には依存しないため省略できる：

  log P(Y = y | λ) ∝ y * log(λ) - λ

  コード内の対応

  ll = np.sum(-lambda_param + y * np.log(lambda_param + 1e-10))
  #             ↑           ↑
  #            -λ      y * log(λ)

  - lambda_param: λ（WORDFISHモデルでは exp(α_i + ψ_j + ω_i * β_j)）
  - y: 観測された単語出現回数
  - 1e-10: log(0) を避けるための微小値（数値安定性）

  複数の観測値がある場合

  文書-単語行列では複数の観測値（複数の文書×単語の組み合わせ）があるため、それぞれの対数尤度を合計する：

  総対数尤度 = Σ[すべての i,j について] (y_ij * log(λ_ij) - λ_ij)

  WORDFISHモデルでの λ の定義

  WORDFISHでは：
  λ_ij = exp(α_i + ψ_j + ω_i * β_j)

  - α_i: 文書 i の固定効果
  - ψ_j: 単語 j の固定効果
  - ω_i: 文書 i の潜在的位置（推定したい主要パラメータ）
  - β_j: 単語 j の重み（識別力パラメータ）

  この λ_ij を上記の対数尤度式に代入して最適化することで、各パラメータを推定


⏺ Z-score変換：モデルの識別問題（identification problem）に対する処方箋

  識別問題とは

  WORDFISHモデルでは、以下の関係が成り立つ：

  λ_ij = exp(α_i + ψ_j + ω_i * β_j)

  この式にはスケールの不定性がある。つまり、任意の定数 c と k に対して：

  ω'_i = c + k * ω_i
  β'_j = β_j / k
  ψ'_j = ψ_j - c * β_j

  と変換しても、λ_ij の値は変わらない（同じ尤度を持つ）

  Z-score変換の目的

  この不定性を解消するため、ω（文書位置）を平均0、標準偏差1に標準化する

  コードの詳細解説

  # 現在のωの平均と標準偏差を計算
  omega_bar = np.mean(params['omega'])
  omega_sd = np.std(params['omega'])

  if omega_sd > 0:
      # βを保存（後でψの調整に使用）
      b1 = params['beta'].copy()

      # βをスケール調整（標準偏差倍する）
      params['beta'] = params['beta'] * omega_sd

      # ωを標準化（平均0、標準偏差1に）
      params['omega'] = (params['omega'] - omega_bar) / omega_sd

      # ψを調整（λが変わらないように）
      params['psi'] = params['psi'] + b1 * omega_bar

  なぜこの変換で λ が変わらないか

  変換前：
  λ_ij = exp(α_i + ψ_j + ω_i * β_j)

  変換後：
  λ'_ij = exp(α_i + (ψ_j + β_j * ω̄) + ((ω_i - ω̄)/σ_ω) * (β_j * σ_ω))
       = exp(α_i + ψ_j + β_j * ω̄ + β_j * (ω_i - ω̄))
       = exp(α_i + ψ_j + β_j * ω_i)
       = λ_ij

  平均と標準偏差の項が相殺されるため、モデルの予測値は変わらない

  この処理の意義

  - 解釈可能性: ω が標準化されることで、文書間の相対的な位置を比較しやすくなる
  - 一意性: パラメータの推定値が一意に定まる
  - 収束の安定性: EMアルゴリズムの各イテレーションで同じスケールを保つことで、収束が安定する

  この標準化は統計モデルの**識別制約（identification constraint）**の一種で、推定可能なパラメータ空間を適切に制限するための重要な処理


⏺ Global identificationは、ωの符号の不定性を解決するための処理

  符号の不定性とは

  WORDFISHモデルでは、以下の変換を行っても λ_ij の値は変わらない：

  ω'_i = -ω_i
  β'_j = -β_j

  なぜなら：
  λ_ij = exp(α_i + ψ_j + ω_i * β_j)
       = exp(α_i + ψ_j + (-ω_i) * (-β_j))

  つまり、ω と β の符号を同時に反転させても、モデルは同じ

  問題点

  この符号の不定性により：

  - 実行するたびに推定結果の符号が反転する可能性がある
  - 文書の位置関係が逆になってしまう（左右が反転）
  - 結果の解釈が困難になる

  コードの詳細解説

  # Global identification (ensure consistent direction)
  if params['omega'][dir_docs[0]] > params['omega'][dir_docs[1]]:
      params['omega'] = -params['omega']
      params['beta'] = -params['beta']

  dir_docs パラメータ

  dir_docs=(0, 1) は、方向を決定するための2つの基準文書のインデックス

  処理の流れ

  1. 基準文書の位置を確認:
    - dir_docs[0]番目の文書（デフォルトは文書0）
    - dir_docs[1]番目の文書（デフォルトは文書1）
  2. 符号の判定:
    - もし ω[0] > ω[1] なら、符号を反転
  3. 符号の反転:
    - ω と β の両方の符号を反転

  具体例

  # 反転前の例
  omega = [1.5, -0.5, -1.0]  # 文書0が右端、文書1が左寄り

  # dir_docs=(0, 1) の場合
  # omega[0]=1.5 > omega[1]=-0.5 なので反転
  omega = [-1.5, 0.5, 1.0]  # 文書0が左端、文書1が右寄り

  この処理の意義

  - 再現性: 同じデータに対して常に同じ符号の結果が得られる
  - 解釈の一貫性: 文書0が文書1より左側（小さい値）に位置すると決めることで、政治的スペクトラム（左派-右派）などの解釈が一貫する
  - 比較可能性: 異なる実行結果や異なるデータセット間での比較が容易になる

  dir_docs の選び方

  実際の分析では、以下のような基準で選ぶ：

  # 例: 政党の位置推定
  dir_docs=(left_party_index, right_party_index)
  # → 左派政党が小さい値、右派政党が大きい値になるよう固定

  この制約により、潜在的な1次元スケール上での文書の順序が一意に定まるようになる

  まとめ

  WORDFISHのような潜在変数モデルでは、数学的には無限に多くの解が存在するため、以下の工夫で実用的なモデルにしている：

  3つの主要な制約

  1. 固定効果の基準設定
    - alpha[0] = 0: 最初の文書の固定効果をゼロに固定
  2. Z-score標準化
    - ωを平均0、標準偏差1に標準化
    - スケールの不定性を解消
  3. 符号の固定
    - 基準文書間の大小関係を固定
    - 方向の不定性を解消

  その他の工夫

  - 正則化: sigmaパラメータでβに正規分布の事前分布を設定
    → prior = -0.5 * (beta**2 / self.sigma**2)
    → βが極端に大きくなるのを防ぐ
  - 数値安定性: log(lambda_param + 1e-10)
    → log(0)によるエラーを回避
  - SVD初期値
    → 良い初期値から開始して収束を速める

  これらの工夫により、理論的には不定なモデルが、実際には安定して解釈可能な結果を出せるようになっている
