⏺ ポアソン分布の対数尤度の導出

  ポアソン分布の確率質量関数

  ポアソン分布 Poisson(λ) に従う確率変数 Y の確率質量関数は：

  P(Y = y | λ) = (λ^y * e^(-λ)) / y!

  ここで、y は観測された単語の出現回数、λ は期待値（平均出現回数）

  対数尤度への変換

  この確率質量関数の対数を取る：

  log P(Y = y | λ) = log(λ^y * e^(-λ) / y!)
                   = log(λ^y) + log(e^(-λ)) - log(y!)
                   = y * log(λ) - λ - log(y!)

  最適化における定数項の省略

  最適化（パラメータ推定）では、パラメータ λ に依存しない項は無視できる
  log(y!) は y（観測データ）にのみ依存し、λ には依存しないため省略できる：

  log P(Y = y | λ) ∝ y * log(λ) - λ

  コード内の対応

  ll = np.sum(-lambda_param + y * np.log(lambda_param + 1e-10))
  #             ↑           ↑
  #            -λ      y * log(λ)

  - lambda_param: λ（WORDFISHモデルでは exp(α_i + ψ_j + ω_i * β_j)）
  - y: 観測された単語出現回数
  - 1e-10: log(0) を避けるための微小値（数値安定性）

  複数の観測値がある場合

  文書-単語行列では複数の観測値（複数の文書×単語の組み合わせ）があるため、それぞれの対数尤度を合計する：

  総対数尤度 = Σ[すべての i,j について] (y_ij * log(λ_ij) - λ_ij)

  WORDFISHモデルでの λ の定義

  WORDFISHでは：
  λ_ij = exp(α_i + ψ_j + ω_i * β_j)

  - α_i: 文書 i の固定効果
  - ψ_j: 単語 j の固定効果
  - ω_i: 文書 i の潜在的位置（推定したい主要パラメータ）
  - β_j: 単語 j の重み（識別力パラメータ）

  この λ_ij を上記の対数尤度式に代入して最適化することで、各パラメータを推定

⏺ Z-score変換：モデルの識別問題（identification problem）に対する処方箋

  識別問題とは

  WORDFISHモデルでは、以下の関係が成り立つ：

  λ_ij = exp(α_i + ψ_j + ω_i * β_j)

  この式にはスケールの不定性がある。つまり、任意の定数 c と k に対して：

  ω'_i = c + k * ω_i
  β'_j = β_j / k
  ψ'_j = ψ_j - c * β_j

  と変換しても、λ_ij の値は変わらない（同じ尤度を持つ）

  Z-score変換の目的

  この不定性を解消するため、ω（文書位置）を平均0、標準偏差1に標準化する

  コードの詳細解説

  # 現在のωの平均と標準偏差を計算
  omega_bar = np.mean(params['omega'])
  omega_sd = np.std(params['omega'])

  if omega_sd > 0:
      # βを保存（後でψの調整に使用）
      b1 = params['beta'].copy()

      # βをスケール調整（標準偏差倍する）
      params['beta'] = params['beta'] * omega_sd

      # ωを標準化（平均0、標準偏差1に）
      params['omega'] = (params['omega'] - omega_bar) / omega_sd

      # ψを調整（λが変わらないように）
      params['psi'] = params['psi'] + b1 * omega_bar

  なぜこの変換で λ が変わらないか

  変換前：
  λ_ij = exp(α_i + ψ_j + ω_i * β_j)

  変換後：
  λ'_ij = exp(α_i + (ψ_j + β_j * ω̄) + ((ω_i - ω̄)/σ_ω) * (β_j * σ_ω))
       = exp(α_i + ψ_j + β_j * ω̄ + β_j * (ω_i - ω̄))
       = exp(α_i + ψ_j + β_j * ω_i)
       = λ_ij

  平均と標準偏差の項が相殺されるため、モデルの予測値は変わらない

  この処理の意義

  - 解釈可能性: ω が標準化されることで、文書間の相対的な位置を比較しやすくなる
  - 一意性: パラメータの推定値が一意に定まる
  - 収束の安定性: EMアルゴリズムの各イテレーションで同じスケールを保つことで、収束が安定する

  この標準化は統計モデルの**識別制約（identification constraint）**の一種で、推定可能なパラメータ空間を適切に制限するための重要な処理

⏺ Global identificationは、ωの符号の不定性を解決するための処理

  符号の不定性とは

  WORDFISHモデルでは、以下の変換を行っても λ_ij の値は変わらない：

  ω'_i = -ω_i
  β'_j = -β_j

  なぜなら：
  λ_ij = exp(α_i + ψ_j + ω_i * β_j)
       = exp(α_i + ψ_j + (-ω_i) * (-β_j))

  つまり、ω と β の符号を同時に反転させても、モデルは同じ

  問題点

  この符号の不定性により：

  - 実行するたびに推定結果の符号が反転する可能性がある
  - 文書の位置関係が逆になってしまう（左右が反転）
  - 結果の解釈が困難になる

  コードの詳細解説

  # Global identification (ensure consistent direction)
  if params['omega'][dir_docs[0]] > params['omega'][dir_docs[1]]:
      params['omega'] = -params['omega']
      params['beta'] = -params['beta']

  dir_docs パラメータ

  dir_docs=(0, 1) は、方向を決定するための2つの基準文書のインデックス

  処理の流れ

  1. 基準文書の位置を確認:
    - dir_docs[0]番目の文書（デフォルトは文書0）
    - dir_docs[1]番目の文書（デフォルトは文書1）
  2. 符号の判定:
    - もし ω[0] > ω[1] なら、符号を反転
  3. 符号の反転:
    - ω と β の両方の符号を反転

  具体例

  # 反転前の例
  omega = [1.5, -0.5, -1.0]  # 文書0が右端、文書1が左寄り

  # dir_docs=(0, 1) の場合
  # omega[0]=1.5 > omega[1]=-0.5 なので反転
  omega = [-1.5, 0.5, 1.0]  # 文書0が左端、文書1が右寄り

  この処理の意義

  - 再現性: 同じデータに対して常に同じ符号の結果が得られる
  - 解釈の一貫性: 文書0が文書1より左側（小さい値）に位置すると決めることで、政治的スペクトラム（左派-右派）などの解釈が一貫する
  - 比較可能性: 異なる実行結果や異なるデータセット間での比較が容易になる

  dir_docs の選び方

  実際の分析では、以下のような基準で選ぶ：

  # 例: 政党の位置推定
  dir_docs=(left_party_index, right_party_index)
  # → 左派政党が小さい値、右派政党が大きい値になるよう固定

  この制約により、潜在的な1次元スケール上での文書の順序が一意に定まるようになる

⏺ まとめ

  WORDFISHのような潜在変数モデルでは、数学的には無限に多くの解が存在するため、以下の工夫で実用的なモデルにしている：

  3つの主要な制約

  1. 固定効果の基準設定
    - alpha[0] = 0: 最初の文書の固定効果をゼロに固定
  2. Z-score標準化
    - ωを平均0、標準偏差1に標準化
    - スケールの不定性を解消
  3. 符号の固定
    - 基準文書間の大小関係を固定
    - 方向の不定性を解消

  その他の工夫

  - 正則化: sigmaパラメータでβに正規分布の事前分布を設定
    → prior = -0.5 * (beta**2 / self.sigma**2)
    → βが極端に大きくなるのを防ぐ
  - 数値安定性: log(lambda_param + 1e-10)
    → log(0)によるエラーを回避
  - SVD初期値
    → 良い初期値から開始して収束を速める

  これらの工夫により、理論的には不定なモデルが、実際には安定して解釈可能な結果を出せるようになっている
